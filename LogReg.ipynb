{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import numpy as np\n",
    "\n",
    "class LogReg:\n",
    "    def __init__(self,dim, l_rate):\n",
    "        dim = max(1, dim)\n",
    "        self.dim = dim\n",
    "        self.l_rate = l_rate # learning rate\n",
    "        self.weights = np.zeros(dim+1)\n",
    "\n",
    "    def init_weights(self):\n",
    "        self.weights = np.zeros(self.dim + 1)\n",
    "        \n",
    "    def reshape_X(self, X):\n",
    "        #number of examples\n",
    "        if len(X.shape) > 1 and X.shape[0] >= 1:\n",
    "            num_ex = X.shape[0]\n",
    "            return np.c_[ np.ones(num_ex), X]\n",
    "        else:\n",
    "            cur_size = X.size\n",
    "            return np.r_[1, X]\n",
    "\n",
    "    def risk_score(self, X):\n",
    "        #should return (n, 1)\n",
    "        res_X = self.reshape_X(X)\n",
    "        my_risk = np.dot(res_X,self.weights)\n",
    "        return my_risk\n",
    "\n",
    "    def sigmoid(self, X):\n",
    "        #theta(s) = e^s/(1+e^s)\n",
    "        cur_es = np.exp(risk_score(X))\n",
    "        return np.divide(cur_es, np.add(1, cur_es))\n",
    "\n",
    "    def gradient(self, X, y):\n",
    "        #grad(E_in) = (-1/N)*sum(n=1;N){(y_n*x_n)/(1+e^(y_n*wT(t)*x_n))}\n",
    "        res_X = self.reshape_X(X)\n",
    "        cur_N = X.shape[0]\n",
    "        cur_numer = np.multiply(y,res_X) #y_n*x_n by row, should be (n,dim+1)\n",
    "        #should return (n,1)\n",
    "        cur_denom = np.add(1, np.exp(np.multiply(y, self.risk_score(X))))\n",
    "        #divide cur_numer row wise by cur_denom, should still be (n, dim+1)\n",
    "        presum = np.divide(cur_numer, cur_denom)\n",
    "        #sum by column\n",
    "        cur_sum = np.sum(presum, axis = 0)\n",
    "        #now normalize by (-1/N) and return\n",
    "        cur_sum = np.divide(cur_sum, -1*cur_N)\n",
    "        return cur_sum\n",
    "\n",
    "    def sto_gradient(self, xn, yn):\n",
    "        #stochastic gradient, should be only one example\n",
    "        res_X = self.reshape_X(xn)\n",
    "        cur_numer = np.multiply(yn, res_X)\n",
    "        cur_denom = np.add(1, np.exp(np.multiply(yn, self.risk_score(xn))))\n",
    "        return np.multiply(-1, np.divide(cur_numer, cur_denom))\n",
    "    \n",
    "    def update_weights(self, X, y):\n",
    "        #w(t+1) = w(t) - l_rate * gradient\n",
    "        cur_grad = self.gradient(X,y)\n",
    "        self.weights = np.subtract(self.weights, np.multiply(self.l_rate, cur_grad))\n",
    "    \n",
    "    def sto_gd(self, X, y):\n",
    "        # a run of stochastic gradient descent\n",
    "        cur_num = X.shape[0]\n",
    "        #get indices for every row/example in X and shuffle them\n",
    "        cur_idxs = np.arange(cur_num)\n",
    "        np.random.shuffle(cur_idxs)\n",
    "        #now update weights one by one\n",
    "        for cur_idx in cur_idxs:\n",
    "            cur_grad = self.sto_gradient(X[cur_idx], y[cur_idx])\n",
    "            self.weights = np.subtract(self.weights, np.multiply(self.l_rate, cur_grad))\n",
    "\n",
    "    def ce_error(self, X, y):\n",
    "        #cross-entropy error\n",
    "        #e_in = (1/N) sum(n=1;N){ ln(1+e^(-yn*wT*xn))}\n",
    "        res_X = self.reshape_X(X)\n",
    "        cur_N = res_X.shape[0]\n",
    "        cur_val = np.log(np.add(1, np.exp(np.multiply(np.multiply(-1,y), self.risk_score(X)))))\n",
    "        #should be (n,1)\n",
    "        return np.divide(np.sum(cur_val), cur_N)\n",
    "        \n",
    "\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Line:\n",
    "    def __init__(self, p1, p2):\n",
    "        self.p1 = p1\n",
    "        self.p2 = p2\n",
    "        diff = np.subtract(p2, p1)\n",
    "        if diff[0] <= 0.0001:\n",
    "            self.slope = None\n",
    "            self.is_vert = True\n",
    "        else:\n",
    "            self.slope = diff[1]/diff[0]\n",
    "            self.is_vert = False\n",
    "        if not self.is_vert:\n",
    "            self.y_int = ((-1 * p1[1])/self.slope) + p1[0]\n",
    "\n",
    "        \n",
    "    def label(self,testpt):\n",
    "        if self.is_vert == False:\n",
    "            line_y = self.slope*testpt[0] + self.y_int\n",
    "            diff = testpt[1] - line_y\n",
    "        else:\n",
    "            line_x = self.p1[0]\n",
    "            diff = testpt[0] - line_x\n",
    "        return np.sign(diff)\n",
    "    \n",
    "    def label_pts(self, ptset):\n",
    "        label_list = np.array([])\n",
    "        for pt in ptset:\n",
    "            label = self.label(pt)\n",
    "            label_list = np.concatenate((label_list, [label]))\n",
    "        return label_list\n",
    "    \n",
    "logreg = LogReg(2, 0.01)         \n",
    "LogR_EXP = 100 \n",
    "LogR_N = 100 \n",
    "LogR_WTHRESH = 0.01 \n",
    "\n",
    "logr_epochs = np.array([]) \n",
    "logr_eout = np.array([])\n",
    "\n",
    "for i in range(LogR_EXP):\n",
    "    logreg.init_weights() \n",
    "    line_pts = np.random.uniform(-1, 1, (2,2))\n",
    "    line = Line(line_pts[0], line_pts[1])\n",
    "    train_pts = np.random.uniform(-1,1,(LogR_N,2))\n",
    "    labels = line.label_pts(train_pts)\n",
    "    \n",
    "    epochs = 0 \n",
    "    wdiff = 1\n",
    "    while wdiff >= LogR_WTHRESH:\n",
    "        epochs = epochs + 1\n",
    "        w_t = logreg.weights \n",
    "        logreg.sto_gd(train_pts, labels) \n",
    "        w_p = logreg.weights \n",
    "        wdiff = np.linalg.norm(np.subtract(w_p, w_t)) \n",
    "        \n",
    "    logr_epochs = np.concatenate((logr_epochs, [epochs])) \n",
    "    \n",
    "    Eouts = np.random.uniform(-1,1, (LogR_N, 2))\n",
    "    Eouts_labels = line.label_pts(Eouts)\n",
    "    eout = logreg.ce_error(Eouts, Eouts_labels)\n",
    "    \n",
    "    logr_eout = np.concatenate((logr_eout,[eout]))\n",
    "\n",
    "logr_epochs_avg = np.average(logr_epochs)\n",
    "logr_eout_avg = np.average(logr_eout)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "302.64"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lr_epochs_avg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.08771792982246518"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lr_eout_avg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
